{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Neural Network For Binary Classification From Scratch\n",
    "\n",
    "## Overview\n",
    "In this repo, I implemented a neural network from scratch, using numpy, as hown in the figure below, we will implement a neural network of 4 fully conntected layer to perform binary classification\n",
    "\n",
    "<!-- <div>\n",
    "<img src='images/multilay_perceptron.png' hieght=500, width=700 >\n",
    "\n",
    "</div> -->\n",
    "\n",
    "\n",
    "Note that in our case, the input layer has 31 features instead of 4 (adapted to our datasest)\n",
    "\n",
    "## Dataset\n",
    "We will be using a tumor dataset witht the targt variable being whether the tumor is bening of malignant\n",
    "It has 32 features and 569 rows.\n",
    "\n",
    "## Architecture of the neural network: \n",
    "The network consists of 4 dense layers (fully connected layers) with the following structure:\n",
    "- Input Layer $( l_0 )$: 31 neurons\n",
    "- Hidden Layer 1 $( l_1 )$: $ 3 $ neurons\n",
    "- Hidden Layer 2 $( l_2 )$: $ 3 $ neurons\n",
    "- Output Layer $( l_4 )$: 2 neurons\n",
    "\n",
    "\n",
    "## Forward pass : \n",
    "\n",
    "During the forward propagation, the activation are calculated as follows : \n",
    "\n",
    "$$ {z}_{l_1} = {W}_{l_0 l_1}^T {a}_{l_0} + {b}_{l_1} $$ \n",
    "$$ \\mathbf{a}_{l_1} = \\sigma(\\mathbf{z}_{l_1}) $$ \n",
    "\n",
    "Note that for the input layer, ${a}_{l_0}= x$\n",
    "\n",
    "- $ \\mathbf{W}_{l_0 l_1}^T $ is of size $(31, 3) $\n",
    "- $\\mathbf{b}_{l_1} $ is of size $(1, 3) $\n",
    "- $\\mathbf{a}_{l_0} $ is of size $ (4)$\n",
    "\n",
    "2. **Hidden Layer 1 to Hidden Layer 2**: <br>\n",
    "$$ \\mathbf{z}_{l_2} = \\mathbf{W}_{l_1 l_2}^T \\mathbf{a}_{l_1} + \\mathbf{b}_{l_2} $$\n",
    "$$\\mathbf{a}_{l_2} = \\sigma(\\mathbf{z}_{l_2})$$\n",
    "\n",
    "- $\\mathbf{W}_{l_1 l_2}^T $ is of size $(3, 3) $\n",
    "- $\\mathbf{b}_{l_2} $ is of size $(3) $\n",
    "- $\\mathbf{a}_{l_1} $ is of size $(3) $\n",
    "\n",
    "\n",
    "3. **Hidden Layer 2 to Output Layer**: <br>\n",
    "$$ \\mathbf{z}_{l_3} = \\mathbf{W}_{l_2 l_3}^T \\mathbf{a}_{l_2} + \\mathbf{b}_{l_3} $$\n",
    "$$ \\mathbf{a}_{l_3} = \\sigma(\\mathbf{z}_{l_3})$$\n",
    "- $ \\mathbf{W}_{l_2 l_3}^T $ is of size $ (3, 2) $\n",
    "- $ \\mathbf{b}_{l_3} $ is of size $ (2) $\n",
    "- $ \\mathbf{a}_{l_2} $ is of size $ (3) $\n",
    "\n",
    "### Activation Function\n",
    "$ \\sigma $ represents the activation function, which can vary (e.g., ReLU, sigmoid, tanh).\n",
    "\n",
    "\n",
    "To recapitulate, the mathematical formulation for the described neural network is:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z}_{l_1} &= \\mathbf{W}_{l_0 l_1}^T \\mathbf{a}_{l_0} + \\mathbf{b}_{l_1}, \\quad & \\mathbf{a}_{l_1} = \\sigma(\\mathbf{z}_{l_1}) \\\\\n",
    "\\mathbf{z}_{l_2} &= \\mathbf{W}_{l_1 l_2}^T \\mathbf{a}_{l_1} + \\mathbf{b}_{l_2}, \\quad & \\mathbf{a}_{l_2} = \\sigma(\\mathbf{z}_{l_2}) \\\\\n",
    "\\mathbf{z}_{l_4} &= \\mathbf{W}_{l_2 l_3}^T \\mathbf{a}_{l_2} + \\mathbf{b}_{l_3}, \\quad & \\mathbf{a}_{l_3} = \\sigma(\\mathbf{z}_{l_3})\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "\n",
    "## Backward pass :\n",
    "## Loss function :\n",
    "We will be using the cross-entropy loss, also known as log loss for binary classification. It measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "Given the true label $ y $ and the predicted probability $ \\hat{y} $, the cross-entropy loss for a single example, representing the **loss function** is defined as:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "For a set of $ N $ examples, the average cross-entropy loss, also known as the **cost function** for all the observations in the dataset, is defined as follows:\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_i $ is the true label of the $ i $-th example (0 or 1).\n",
    "- $ \\hat{y}_i $ is the predicted probability for the $ i$-th example.\n",
    "- $ N $ is the number of examples.\n",
    "\n",
    "## Optimizer :\n",
    "\n",
    "We will use the **Graidient descent** which is an iterative optimization algorithm used to minimize the loss function during back propagation. It updates the model parameters in the direction of the negative gradient of the loss function with respect to the parameters.\n",
    "\n",
    "Given a loss function $ L(\\theta) $ where $ \\theta $ represents the model parameters, the gradient descent update rule is defined as:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\theta $ denotes the model parameters.\n",
    "- $ \\eta $ is the learning rate, a hyperparameter that controls the step size of the update.\n",
    "- $ \\nabla_{\\theta} L(\\theta) $ is the gradient of the loss function with respect to the parameters $ \\theta $.\n",
    "\n",
    "For each iteration, the algorithm performs the following steps:\n",
    "1. Compute the gradient of the loss function with respect to each parameter.\n",
    "2. Update each parameter by subtracting the product of the learning rate and the gradient.\n",
    "\n",
    "The process is repeated iteratively until convergence, i.e., until the parameters stabilize or the loss function reaches a minimum value.\n",
    "\n",
    "\n",
    "In mathematical terms, the parameter update rule at iteration $ t $ can be expressed as:\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\eta \\nabla_{\\theta} L(\\theta_{t-1})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\theta_t $ is the updated parameter at iteration $ t $.\n",
    "- $ \\theta_{t-1} $ is the parameter value from the previous iteration.\n",
    "- $ \\eta $ is the learning rate.\n",
    "\n",
    "Gradient descent can be applied in various forms, including:\n",
    "- **Batch Gradient Descent**: Uses the entire dataset to compute the gradient.\n",
    "- **Stochastic Gradient Descent (SGD)**: Uses a single data point to compute the gradient.\n",
    "- **Mini-batch Gradient Descent**: Uses a small subset (mini-batch) of the dataset to compute the gradient. <br>\n",
    "In our case, we will use all the observations to update the gradients.\n",
    "\n",
    "### Activation function : \n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "#### Derivative\n",
    "In order to find the derivative of the sigmoid function, we follow these steps : \n",
    "\n",
    "1. Write the sigmoid function:\n",
    "\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "2. Differentiate the sigmoid function using the chain rule:\n",
    "\n",
    "   First, let's denote $y = \\sigma(z)$:\n",
    "\n",
    "   $$\n",
    "   y = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "   To find $\\frac{dy}{dz}$, we use the chain rule. We can rewrite $y$ as:\n",
    "\n",
    "   $$\n",
    "   y = (1 + e^{-z})^{-1}\n",
    "   $$\n",
    "\n",
    "3. Apply the chain rule:\n",
    "\n",
    "   Let $$u = 1 + e^{-z}$$\n",
    "   Then $$ y = u^{-1} $$.\n",
    "\n",
    "   The derivative of $u^{-1}$ with respect to $u$ is:\n",
    "\n",
    "   $$\n",
    "   \\frac{d}{du} (u^{-1}) = -u^{-2}\n",
    "   $$\n",
    "\n",
    "   Now, we need $\\frac{du}{dz}$:\n",
    "\n",
    "   $$\n",
    "   u = 1 + e^{-z}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\frac{du}{dz} = -e^{-z}\n",
    "   $$\n",
    "\n",
    "   Using the chain rule:\n",
    "\n",
    "   $$\n",
    "   \\frac{dy}{dz} = \\frac{dy}{du} \\cdot \\frac{du}{dz} = (-u^{-2}) \\cdot (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2}\n",
    "   $$\n",
    "\n",
    "4. Simplify the expression:\n",
    "\n",
    "   Recall that $y = \\sigma(z) = \\frac{1}{1 + e^{-z}}$. We can substitute back to simplify:\n",
    "\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "   Hence,\n",
    "\n",
    "   $$\n",
    "   \\frac{dy}{dz} = \\frac{e^{-z}}{(1 + e^{-z})^2}\n",
    "   $$\n",
    "\n",
    "5. Express the derivative in terms of $\\sigma(z)$:\n",
    "\n",
    "   We can rewrite the numerator $e^{-z}$ as:\n",
    "\n",
    "   $$\n",
    "   e^{-z} = \\frac{1}{e^z}\n",
    "   $$\n",
    "\n",
    "   So the derivative becomes:\n",
    "\n",
    "   $$\n",
    "   \\frac{dy}{dz} = \\frac{\\frac{1}{e^z}}{(1 + e^{-z})^2}\n",
    "   $$\n",
    "\n",
    "   Notice that:\n",
    "\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}} \\quad \\text{and} \\quad 1 - \\sigma(z) = 1 - \\frac{1}{1 + e^{-z}} = \\frac{e^{-z}}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "   Therefore:\n",
    "\n",
    "   $$\n",
    "   \\frac{dy}{dz} = \\sigma(z) (1 - \\sigma(z))\n",
    "   $$\n",
    "\n",
    "Thus, we have shown that the derivative of the sigmoid function $\\sigma(z)$ is:\n",
    "\n",
    "$$\n",
    "\\frac{d\\sigma(z)}{dz} = \\sigma(z) (1 - \\sigma(z))\n",
    "$$\n",
    "\n",
    "### Gradient calculations : \n",
    "Here we explicit how the gradients are being caluclated : \n",
    "\n",
    "#### For the output layer : \n",
    "The derivative of the cross entropy loss function with respect to the prediction $\\hat y$ is calcualted as follows : \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat y} = \\frac{-y}{a_{l3}} + \\frac{1-y}{1-a_{l3}}\n",
    "$$\n",
    "Knowing that for the output layer, $\\hat y = a_{l3}$, the derivative becomes : \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat a_{l3}} = \\frac{-y}{a_{l3}} + \\frac{1-y}{1-a_{l3}}\n",
    "$$\n",
    "We also know that :\n",
    "$$ a_{l3} = \\sigma (z_{l_3})$$\n",
    "$$ \\frac{\\partial \\hat y}{\\partial z_{l_3}} = a_{l3}(1- a_{l3})$$\n",
    "By applying the chain rule for partial derivatives : \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{l_3}} = \\frac{\\partial L}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial z_3} $$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{l_3} }= (\\frac{-y}{a_{l3}} + \\frac{1-y}{1-a_{l3}})(a_{l3}(1- a_{l3}))\n",
    "$$\n",
    "\n",
    "After putting them with the same denominator we obtain \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{l_3} }= -y(1-a_{l3}) + a_{l3}(1-y)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{l_3}} = -y + a_{l3} y + a_{l3} - a_{l3} y\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{l_3}} = a_{l3} - y \n",
    "$$\n",
    "\n",
    "#### The second hidden layer\n",
    "We know that $$ z_{l_3} = W_{l_2 l_3} a_2 + b_{l_3}$$  $$ a_{l3} = \\sigma (z_{l_3})$$\n",
    "The gradients are as follows : \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{l_2 l_3}} = \\frac{\\partial L}{\\partial z_3} \\frac{\\partial z_3}{\\partial W_{l_2 l_3}} $$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{l_2 l_3}} = a_2 (a_{l3} - y)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_3} = \\frac{\\partial L}{\\partial z_3} \\frac{\\partial z_3}{\\partial b_3} $$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_3} = \\frac{\\partial L}{\\partial z_3} \\frac{\\partial z_3}{\\partial b_3} $$\n",
    "\n",
    "\n",
    "#### The first hidden layer\n",
    "We know that   $$ z_{l_2} = W_{l_1 l_2} a_1 + b_{l_2}$$  $$ a_2 = \\sigma (z_{l_2})$$\n",
    "The gradeints are as follows : \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_2} = \\frac{\\partial L}{\\partial z_{l_3}} \\frac{\\partial z_3}{\\partial a_2} \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_2} = (a_{l3} - y) W_{l_2 l_3} \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{l2}} = \\frac{\\partial L}{\\partial a_2 } \\frac{\\partial a_2}{\\partial z_{l_2}} \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{l_1 l_2}} = a_1 \\frac{\\partial L}{\\partial z_{l_2}} \n",
    "$$ \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_2} = a_1 \\frac{\\partial L}{\\partial z_{l_2}} \n",
    "$$\n",
    "\n",
    "#### The input layer\n",
    "We know that $$ z_{l_1} = W_{l_0 l_1} a_0 + b_{l_1}$$ $$ z_{l_1} = W_{l_0 l_1} X + b_{l_1}$$ and  $$ a_1 = \\sigma (z_{l_1})$$\n",
    "The gradients are as follows :\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_1} = \\frac{\\partial L}{\\partial z_{l_2}} \\frac{\\partial z_2}{\\partial a_1} \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_1} = \\frac{\\partial L}{\\partial z_{l_2}} W_{l_1 l_2} \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{l_1}} = \\frac{\\partial L}{\\partial a_1} \\frac{\\partial a_1}{\\partial z_1} \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{l_0 l_1}} = \\frac{\\partial L}{\\partial z_{l_1}} \\frac{\\partial z_{l_1}}{\\partial W_{l_0 l_1}} \n",
    "$$\n",
    "\n",
    "\n",
    "## Results : \n",
    "After training the neural network for **4280** epochs with a **learning rate** of **0.9**, we get the folling results\n",
    "\n",
    "|`Training loss`| `Evaluation loos`|\n",
    "|-------------  |----------------  |\n",
    "|**0.0201**     | **0.0899**       |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
